{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from train_models import create_avg_dataset\n",
    "from collections import OrderedDict\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.externals import joblib\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.applications.vgg16 import VGG16\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.46050834655762 seconds to read csv's and create all dataframes\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH_DATA = '/media/ivan/DDE/datasets_proyecto_mt'\n",
    "train_data_csv_paths = ['/datasets/en_train.csv', './datasets/es_train.csv', './datasets/ar_train.csv']\n",
    "train_labels_json_paths = ['./authors_labels/en_train_labels.json', './authors_labels/es_train_labels.json','./authors_labels/ar_train_labels.json']\n",
    "X_dataframes = [None for i in range(len(train_data_csv_paths))]\n",
    "y_dataframes = [None for i in range(len(train_labels_json_paths))]\n",
    "list_of_ids = [None for i in range(len(X_dataframes))]\n",
    "authors_ids = []\n",
    "i = 0\n",
    "t_start = time()\n",
    "for path_x, path_y in zip(train_data_csv_paths, train_labels_json_paths):\n",
    "    list_of_ids[i] = path_x.strip().split('/')[2][:2]\n",
    "    X_dataframes[i] = pd.read_csv(BASE_PATH_DATA + path_x, sep='\\s*,\\s*', header=0, encoding='ascii', engine='python')\n",
    "#     X, y_dataframes[i] = create_avg_dataset(X, path_y)\n",
    "#     y_pred_dict = OrderedDict([(\"class\", y_dataframes[i])])\n",
    "#     X_dataframes[i] = pd.concat([X.reset_index(drop=True), pd.DataFrame.from_dict(y_pred_dict)], axis=1)\n",
    "    i += 1\n",
    "print(\"{} seconds to read csv's and create all dataframes\".format(time() - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"en\", \"es\", \"ar\"]\n",
    "# interesting_labels = set([\"miniskirt\", \"velvet\", \"hair_spray\", \"maillot.1\", \"wig\", \"bikini\", \"stole\", \"brassiere\", \"scoreboard\", \"feather_boa\", \"bath_towel\", \"airliner\", \"silky_terrier\", \"bonnet\", \"bulletproof_vest\", \"maillot\", \"Model_T\", \"sarong\", \"overskirt\", \"shower_curtain\", \"hoopskirt\", \"lipstick\", \"plate_rack\", \"hand_blower\", \"Lhasa\", \"wig\", \"velvet\", \"bath_towel\", \"lipstick\", \"feather_boa\", \"bonnet\", \"face_powder\", \"stole\", \"tub\", \"milk_can\", \"bathtub\", \"overskirt\", \"golden_retriever\", \"Angora\", \"birdhouse\", \"fur_coat\", \"bikini\", \"Maltese_dog\", \"brassiere\", \"hair_spray\", \"lawn_mower\", \"ice_lolly\", \"maillot.1\", \"hair_slide\", \"chain_saw\", \"ballplayer\", \"velvet\", \"cradle\", \"brassiere\", \"broccoli\", \"lynx\", \"red-backed_sandpiper\", \"chiffonier\", \"Border_terrier\", \"lipstick\", \"miniskirt\", \"African_chameleon\", \"bath_towel\", \"safety_pin\", \"sea_urchin\", \"vase\", \"lighter\", \"football_helmet\", \"snorkel\", \"coyote\", \"ant\", \"pot\", \"shield\", \"military_uniform\", \"paddle\"])\n",
    "# interesting_labels = set([\"brassiere\", \"velvet\", \"bath_towel\", \"lipstick\"])\n",
    "interesting_labels = set([\"ballplayer\"])\n",
    "with open(\"authors_candidates_for_classification_only_ball.txt\", \"w\") as file:\n",
    "    threshold = 0.80\n",
    "    for i in range(len(X_dataframes)):\n",
    "        file.write(labels[i] + \"\\n\")\n",
    "        for index, row in X_dataframes[i].iterrows():\n",
    "            for label in interesting_labels:\n",
    "                if row[label] > threshold:\n",
    "                    file.write(\"author_id = {} image_idx = {} class = {}\\n\".format(row['author_id'], index,row['class']))\n",
    "                    file.write(\"{} = {} \\n\".format(label, row[label]))\n",
    "                    file.write(\"{}\\n\".format(decode_predictions(np.array([row.drop([\"author_id\", \"class\"])]), top=3)[0]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = VGG16(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_image_representation(image_representation):\n",
    "    models_dir = './models/'\n",
    "    models_files_paths = [join(models_dir, f) for f in listdir(models_dir) if isfile(join(models_dir, f))]\n",
    "    loaded_models = [joblib.load(model_path) for model_path in models_files_paths]\n",
    "    predicted_gender = []\n",
    "    genders = {\"female\" : 0, \"male\" : 0}\n",
    "    model_male_predictor = []\n",
    "    model_female_predictor = []\n",
    "    for model_idx in range(len(loaded_models)):\n",
    "        models_split = models_files_paths[model_idx].replace(models_dir, \"\") \n",
    "        models_split = models_files_paths[model_idx].strip().split('_')\n",
    "        ids_models = []\n",
    "        for s in models_split:\n",
    "            if s == \"svm\":\n",
    "                break\n",
    "            ids_models.append(s)\n",
    "        ids_models[0] = ids_models[0][-2:]\n",
    "#         print(\"Using trained model {}\".format(ids_models))\n",
    "        y = loaded_models[model_idx].predict(image_representation)[0]\n",
    "        genders[y] += 1\n",
    "        if y == \"female\":\n",
    "            model_female_predictor.append(\"-\".join(ids_models))\n",
    "    if genders[\"male\"] == 7:\n",
    "        print(\"Todos los modelos predijeron:\\nhombre\")\n",
    "    else:\n",
    "        print(\"Excepto: {}\".format(\", \".join(model_female_predictor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ivan/Escritorio/ballplayer/es/242e15635ee82ffcfca39e31ccf0165d.0.jpeg\n",
      "Atributos (top 3): \n",
      "* soccer_ball\n",
      "* football_helmet\n",
      "* parachute\n",
      "Todos los modelos predijeron:\n",
      "hombre\n",
      "/home/ivan/Escritorio/ballplayer/es/999ce7ac3ae2b7ee747e68158e96d70d.1.jpeg\n",
      "Atributos (top 3): \n",
      "* ballplayer\n",
      "* baseball\n",
      "* football_helmet\n",
      "Todos los modelos predijeron:\n",
      "hombre\n",
      "/home/ivan/Escritorio/ballplayer/es/b3560fad2e4486269f56e012b2bf3918.8.jpeg\n",
      "Atributos (top 3): \n",
      "* ballplayer\n",
      "* baseball\n",
      "* football_helmet\n",
      "Todos los modelos predijeron:\n",
      "hombre\n"
     ]
    }
   ],
   "source": [
    "img_dir = '/home/ivan/Escritorio/ballplayer/es'\n",
    "img_files = [join(img_dir, f) for f in listdir(img_dir) if isfile(join(img_dir, f))]\n",
    "for image_path in img_files:\n",
    "    print(image_path)\n",
    "    image = load_img(image_path, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = preprocess_input(image)\n",
    "    features = MODEL.predict(image)\n",
    "    decoded = [t[1] for t in decode_predictions(features, top=3)[0]]\n",
    "    print('Atributos (top 3): \\n* {}'.format(\"\\n* \".join(decoded)))\n",
    "    evaluate_image_representation(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
